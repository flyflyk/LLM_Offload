INFO - [__main__] - ln:86 - --- Starting Execution (Default) ---
INFO - [__main__] - ln:87 - Using device: cuda
INFO - [__main__] - ln:88 - Batch size: 4
INFO - [__main__] - ln:89 - Loading model 'tiiuae/falcon-rw-1b'...
INFO - [model_loader] - ln:14 - Attempting to load model: tiiuae/falcon-rw-1b (Default (Full Load))
INFO - [model_loader] - ln:27 - Loading tokenizer for tiiuae/falcon-rw-1b...
tokenizer_config.json: 100%|█████████████████████████████████████████| 234/234 [00:00<00:00, 564kB/s]
vocab.json: 798kB [00:00, 27.8MB/s]
merges.txt: 456kB [00:00, 35.3MB/s]
tokenizer.json: 2.11MB [00:00, 64.8MB/s]
special_tokens_map.json: 100%|█████████████████████████████████████| 99.0/99.0 [00:00<00:00, 317kB/s]
INFO - [model_loader] - ln:56 - Layer offloading disabled.
INFO - [model_loader] - ln:58 - Loading model weights for tiiuae/falcon-rw-1b with kwargs: {'low_cpu_mem_usage': True, 'torch_dtype': torch.float16}...
config.json: 1.05kB [00:00, 2.77MB/s]
pytorch_model.bin: 100%|█████████████████████████████████████████| 2.62G/2.62G [00:24<00:00, 106MB/s]
generation_config.json: 100%|███████████████████████████████████████| 115/115 [00:00<00:00, 1.17MB/s]
model.safetensors:  11%|████▌                                     | 283M/2.62G [00:02<00:21, 110MB/s]INFO - [model_loader] - ln:68 - Model set to evaluation mode.
INFO - [model_loader] - ln:71 - Model and tokenizer ready! Loading took 33.36 seconds.
INFO - [model_loader] - ln:72 - ------------------------------
WARNING - [__main__] - ln:93 - Tokenizer does not have a pad_token. Setting pad_token to eos_token.
INFO - [__main__] - ln:106 - 
--- Running Measured Inference Examples in Batches ---
INFO - [__main__] - ln:117 - Processing Batch 1 with 4 prompts...
INFO - [inference_runner] - ln:33 - Input IDs shape after tokenization (batch_size, seq_len): torch.Size([4, 48])
INFO - [inference_runner] - ln:38 - Inputs moved to device: cuda:0
INFO - [inference_runner] - ln:45 - VRAM Before Generate (Device cuda) - Allocated: 2.44 GB, Reserved: 2.49 GB
INFO - [inference_runner] - ln:51 - Running batched inference...
model.safetensors:  16%|██████▌                                   | 409M/2.62G [00:03<00:19, 113MB/s]INFO - [inference_runner] - ln:74 - VRAM After Generate (Device cuda) - Allocated: 2.45 GB
INFO - [inference_runner] - ln:106 - Batch generated 80 new tokens in total.
INFO - [inference_runner] - ln:107 - Total generation time for batch: 1.1106 seconds.
INFO - [inference_runner] - ln:108 - Average latency per new token in batch: 0.0139 seconds/token.
INFO - [inference_runner] - ln:109 - Batch throughput: 72.03 tokens/second.
INFO - [inference_runner] - ln:114 - ------------------------------
INFO - [__main__] - ln:15 - 
--- Results Summary ---
INFO - [__main__] - ln:21 - Model VRAM Footprint:
INFO - [__main__] - ln:24 -   initial_allocated_gb: 0.0000 GB
INFO - [__main__] - ln:24 -   initial_reserved_gb: 0.0000 GB
INFO - [__main__] - ln:24 -   after_load_allocated_gb: 2.4431 GB
INFO - [__main__] - ln:24 -   model_footprint_gb: 2.4431 GB
INFO - [__main__] - ln:25 - ---------------
INFO - [__main__] - ln:27 - Inference Metrics per Batch:
INFO - [__main__] - ln:32 - Batch 1:
INFO - [__main__] - ln:38 -   Prompts in batch: 4
INFO - [__main__] - ln:39 -   Tokens generated: 80
INFO - [__main__] - ln:40 -   Batch processing time: 1.1146 s
INFO - [__main__] - ln:43 -   Avg Latency: 0.0139 sec/token
INFO - [__main__] - ln:51 -   Batch Throughput: 71.77 tokens/sec
INFO - [__main__] - ln:56 -   Inference Peak VRAM Allocated Increase: 0.0878 GB
INFO - [__main__] - ln:57 -   Inference Peak VRAM Allocated Total: 2.5309 GB
INFO - [__main__] - ln:60 - ----------
INFO - [__main__] - ln:62 - 
--- Overall Performance ---
INFO - [__main__] - ln:63 - Total prompts processed: 4
INFO - [__main__] - ln:64 - Total new tokens generated across all batches: 80
INFO - [__main__] - ln:65 - Total processing time for all batches: 1.1146 seconds
INFO - [__main__] - ln:70 - Overall Average Latency: 0.0139 sec/token
INFO - [__main__] - ln:71 - Overall Throughput: 71.77 tokens/sec
INFO - [__main__] - ln:77 - Average of Batch Latencies: 0.0139 sec/token
INFO - [__main__] - ln:79 - ------------------------------
