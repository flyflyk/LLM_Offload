INFO - [__main__] - ln:86 - --- Starting Execution (Streaming) ---
INFO - [__main__] - ln:87 - Using device: cuda
INFO - [__main__] - ln:88 - Batch size: 4
INFO - [__main__] - ln:89 - Loading model 'facebook/opt-6.7b'...
INFO - [model_loader] - ln:14 - Attempting to load model: facebook/opt-6.7b (Streaming (Layer Offload))
INFO - [model_loader] - ln:27 - Loading tokenizer for facebook/opt-6.7b...
INFO - [model_loader] - ln:36 - Accelerate layer offloading (device_map='auto') enabled.
INFO - [model_loader] - ln:38 - Cleaning up existing offload directory: offload_dir
INFO - [model_loader] - ln:41 - Created offload directory: offload_dir
INFO - [model_loader] - ln:58 - Loading model weights for facebook/opt-6.7b with kwargs: {'low_cpu_mem_usage': True, 'torch_dtype': torch.float16, 'device_map': 'auto', 'offload_folder': 'offload_dir', 'max_memory': {0: '10GiB', 'cpu': '8 GiB'}}...
Loading checkpoint shards: 100%|███████████████████████████████████████| 2/2 [00:06<00:00,  3.11s/it]
WARNING - [accelerate.big_modeling] - ln:442 - Some parameters are on the meta device because they were offloaded to the cpu.
INFO - [model_loader] - ln:62 - Model device map (first few layers):
{'model.decoder.embed_tokens': 0, 'lm_head': 0, 'model.decoder.embed_positions': 0, 'model.decoder.final_layer_norm': 0, 'model.decoder.layers.0': 0, 'model.decoder.layers.1': 0, 'model.decoder.layers...
INFO - [model_loader] - ln:68 - Model set to evaluation mode.
INFO - [model_loader] - ln:71 - Model and tokenizer ready! Loading took 9.27 seconds.
INFO - [model_loader] - ln:72 - ------------------------------
INFO - [__main__] - ln:106 - 
--- Running Measured Inference Examples in Batches ---
INFO - [__main__] - ln:117 - Processing Batch 1 with 4 prompts...
INFO - [inference_runner] - ln:33 - Input IDs shape after tokenization (batch_size, seq_len): torch.Size([4, 49])
INFO - [inference_runner] - ln:38 - Inputs moved to device: cuda:0
INFO - [inference_runner] - ln:45 - VRAM Before Generate (Device cuda) - Allocated: 9.40 GB, Reserved: 9.40 GB
INFO - [inference_runner] - ln:51 - Running batched inference...
INFO - [inference_runner] - ln:74 - VRAM After Generate (Device cuda) - Allocated: 9.41 GB
INFO - [inference_runner] - ln:106 - Batch generated 80 new tokens in total.
INFO - [inference_runner] - ln:107 - Total generation time for batch: 12.6813 seconds.
INFO - [inference_runner] - ln:108 - Average latency per new token in batch: 0.1585 seconds/token.
INFO - [inference_runner] - ln:109 - Batch throughput: 6.31 tokens/second.
INFO - [inference_runner] - ln:114 - ------------------------------
INFO - [__main__] - ln:15 - 
--- Results Summary ---
INFO - [__main__] - ln:21 - Model VRAM Footprint:
INFO - [__main__] - ln:24 -   initial_allocated_gb: 0.0000 GB
INFO - [__main__] - ln:24 -   initial_reserved_gb: 0.0000 GB
INFO - [__main__] - ln:24 -   after_load_allocated_gb: 9.4016 GB
INFO - [__main__] - ln:24 -   model_footprint_gb: 9.4016 GB
INFO - [__main__] - ln:25 - ---------------
INFO - [__main__] - ln:27 - Inference Metrics per Batch:
INFO - [__main__] - ln:32 - Batch 1:
INFO - [__main__] - ln:38 -   Prompts in batch: 4
INFO - [__main__] - ln:39 -   Tokens generated: 80
INFO - [__main__] - ln:40 -   Batch processing time: 12.6940 s
INFO - [__main__] - ln:43 -   Avg Latency: 0.1585 sec/token
INFO - [__main__] - ln:51 -   Batch Throughput: 6.30 tokens/sec
INFO - [__main__] - ln:56 -   Inference Peak VRAM Allocated Increase: 0.2746 GB
INFO - [__main__] - ln:57 -   Inference Peak VRAM Allocated Total: 9.6762 GB
INFO - [__main__] - ln:60 - ----------
INFO - [__main__] - ln:62 - 
--- Overall Performance ---
INFO - [__main__] - ln:63 - Total prompts processed: 4
INFO - [__main__] - ln:64 - Total new tokens generated across all batches: 80
INFO - [__main__] - ln:65 - Total processing time for all batches: 12.6940 seconds
INFO - [__main__] - ln:70 - Overall Average Latency: 0.1587 sec/token
INFO - [__main__] - ln:71 - Overall Throughput: 6.30 tokens/sec
INFO - [__main__] - ln:77 - Average of Batch Latencies: 0.1585 sec/token
INFO - [__main__] - ln:79 - ------------------------------
